## Modelの解釈
**Encoder layer**  
手法: 単語埋め込み/Bidirectional-LSTM  
目的: 単語埋め込みを行いつつ、双方向からの全てのトークンの隠れ層出力する。さらに、双方向側の最後の隠れ層による影響値を単語埋め込みベクトルに取り入れる。  
![lstm-image](https://camo.qiitausercontent.com/8d50ed97d4cff5f2d9348034f41896464958fa5c/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3234303939392f37383539626537652d313366632d633465322d613762392d3932643966323130343561362e706e67)  

**self Attention layer**  
手法: アテンション層  
目的: 双方向隠れ層からトークンごとの関係性を確率に変換することで重みとする。これを入力に加えて分類サイズを出力する。  


**Attentionによる可視化**  
トークン同士の関係性(token * token)を一本のベクトルにまとめたもの。(batch, 1, tokne)  
つまり入力値に対する各トークン同士の関係を一つに集約しているといえる。  
![](./photos/sample.png)  

**Limeによる説明性**  
ある文章を入力することで、それを基としていくつかのダミーデータを作成する。モデルから分類することで関係性を生成する。  
![](./photos/sample0.png)  